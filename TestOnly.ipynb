{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(y_pred, y_true):\n",
    "    ## YOUR CODE HERE...\n",
    "    n = len(y_pred)\n",
    "    fps,tps = 0,0\n",
    "    for i in range(n):\n",
    "        if y_pred[i] > y_true[i]:\n",
    "            fps+=1\n",
    "            continue;\n",
    "#         if y_pred[i] < y_true[i]:\n",
    "#             fns+=1\n",
    "#             continue\n",
    "#         if (y_pred[i] + y_true[i]) == 0:\n",
    "#             tns+=1\n",
    "#             continue\n",
    "        if (y_pred[i] + y_true[i]) > 1:\n",
    "            tps+=1\n",
    "            continue\n",
    "    precision = tps/(tps + fps)\n",
    "    return precision\n",
    "\n",
    "def get_recall(y_pred, y_true):\n",
    "    ## YOUR CODE HERE...\n",
    "    n = len(y_pred)\n",
    "    tps, fns = 0,0\n",
    "    for i in range(n):\n",
    "#         if y_pred[i] > y_true[i]:\n",
    "#             fps+=1\n",
    "#             continue;\n",
    "        if y_pred[i] < y_true[i]:\n",
    "            fns+=1\n",
    "            continue\n",
    "#         if (y_pred[i] + y_true[i]) == 0:\n",
    "#             tns+=1\n",
    "#             continue\n",
    "        if (y_pred[i] + y_true[i]) > 1:\n",
    "            tps+=1\n",
    "            continue\n",
    "    recall = tps/(fns + tps)\n",
    "    return recall\n",
    "\n",
    "def get_fscore(y_pred, y_true):\n",
    "    ## YOUR CODE HERE...\n",
    "    recall = get_recall(y_pred, y_true)\n",
    "    precision = get_precision(y_pred, y_true)\n",
    "    fscore = 2/(1/precision + 1/recall)\n",
    "    return fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [1,0,1,0,1,0,1,0]\n",
    "y_true = [0,1,0,0,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "0.5\n",
      "0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "print(get_recall(y_pred,y_true))\n",
    "print(get_precision(y_pred,y_true))\n",
    "print(get_fscore(y_pred,y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(data_file, sure):\n",
    "    words = []\n",
    "    labels = []\n",
    "    with open(data_file,'rt',encoding=\"utf8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if i > 0:\n",
    "                line_split = line[:-1].split(\"\\t\")\n",
    "                words.append(line_split[0].lower())\n",
    "                if sure:\n",
    "                    labels.append(int(line_split[1]))\n",
    "            i += 1\n",
    "    return words,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "from collections import defaultdict\n",
    "from statistics import median\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import syllables\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_complex(datafile):\n",
    "    words, labels = load_file(datafile)\n",
    "    #preds = np.ones_like(labels)\n",
    "    preds = [1]*len(labels)\n",
    "    labels = [1 if i == 0 else 0 for i in labels]\n",
    "    precision = get_precision(preds,labels)\n",
    "    recall = get_recall(preds, labels)\n",
    "    fscore = get_fscore(preds, labels)\n",
    "    performance = [precision,recall,fscore]\n",
    "    return performance\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_file() missing 1 required positional argument: 'sure'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-5995f2c929bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/complex_words_development.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-89c6a1e568a5>\u001b[0m in \u001b[0;36mall_complex\u001b[0;34m(datafile)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mall_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m#preds = np.ones_like(labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: load_file() missing 1 required positional argument: 'sure'"
     ]
    }
   ],
   "source": [
    "print(all_complex(\"data/complex_words_development.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for labeling word length \n",
    "def length_threshold_feature(words, threshold):\n",
    "    return [1 if len(i)>= threshold else 0 for i in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_length_threshold(training_file, development_file):\n",
    "    # t stands for training_file, d stands for development_file\n",
    "    w_t, l_t = load_file(training_file)\n",
    "    w_d, l_d = load_file(development_file)\n",
    "    # find the max fscore during the iteration\n",
    "    fscore_max_t = 0\n",
    "#     fscore_max_d = 0\n",
    "    precision_t_max = 0\n",
    "#     precision_d_max = 0\n",
    "    recall_t_max = 0\n",
    "#     recall_d_max = 0\n",
    "    threshold_t = 0\n",
    "#     threshold_d = 0\n",
    "    for i in range(10):\n",
    "        pred_t = length_threshold_feature(w_t,i)\n",
    "        precision_t,recall_t,fscore_t = get_precision(pred_t,l_t),get_recall(pred_t,l_t),get_fscore(pred_t,l_t)\n",
    "        if fscore_t > fscore_max_t:\n",
    "            fscore_max_t = fscore_t\n",
    "            precision_t_max = precision_t\n",
    "            recall_t_max = recall_t\n",
    "            threshold_t = i\n",
    "#         if fscore_d > fscore_max_d:\n",
    "#             fscore_max_d = fscore_d\n",
    "#             precision_d_max = precision_d\n",
    "#             recall_d_max = recall_d\n",
    "#             threshold_d = i\n",
    "#         print(\"Threshold :%d ,precision in training: %f ,recall in t: %f,fcore in t: %f\"% (i,precision_t,recall_t,fscore_t))\n",
    "#         print(\"Threshold :%d ,precision in dev: %f ,recall in dev: %f,fcore in dev: %f\"% (i,precision_d,recall_d,fscore_d))\n",
    "    pred_d = length_threshold_feature(w_d,threshold_t)\n",
    "    precision_d,recall_d,fscore_d = get_precision(pred_d,l_d),get_recall(pred_d,l_d),get_fscore(pred_d,l_d)\n",
    "    print(\"Best performance in training, threshold : %d, precision %f, recall %f, fscore %f \"%(threshold_t,precision_t_max,recall_t_max,fscore_max_t))\n",
    "    print(\"Best performance in development, threshold : %d, precision %f, recall %f, fscore %f \"%(threshold_t,precision_d,recall_d,fscore_d))\n",
    "    training_performance = [precision_t, recall_t, fscore_t]\n",
    "    development_performance = [precision_d, recall_d, fscore_d]\n",
    "    return training_performance, development_performance    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = \"data/complex_words_training.txt\"\n",
    "development_file = \"data/complex_words_development.txt\"\n",
    "test_file = \"data/complex_words_test_unlabeled.txt\"\n",
    "# print(word_length_threshold(training_file,development_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load Google Ngram counts\n",
    "def load_ngram_counts(ngram_counts_file):\n",
    "    counts = defaultdict(int)\n",
    "    with gzip.open(ngram_counts_file,'rt', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            token,count = line.strip().split('\\t')\n",
    "            if token[0].islower():\n",
    "                counts[token] = int(count)\n",
    "    return counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_threshold_feature(words, threshold, counts):\n",
    "    result = []\n",
    "    for i in range(len(words)):\n",
    "        wordFreq = counts.get(words[i].lower())\n",
    "        if wordFreq is None:\n",
    "            result.append(1)\n",
    "        elif wordFreq <= threshold:\n",
    "            result.append(1)\n",
    "        elif wordFreq > threshold:\n",
    "            result.append(0)\n",
    "    return  result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequency_threshold(training_file, development_file, counts):\n",
    "    \n",
    "    twords, tlabels = load_file(training_file,True)\n",
    "    dwords, dlabels = load_file(development_file,True)\n",
    "    max_in_t = find_max_frequency(counts, twords)\n",
    "    max_in_d = find_max_frequency(counts, dwords)\n",
    "    min_in_t = find_min_frequency(counts, twords)\n",
    "    fscore_max = 0\n",
    "    freq = 0\n",
    "#     for i in range(100000, 200000000 + 10,2000):\n",
    "    preds = frequency_threshold_feature(twords,25000000, counts)\n",
    "    fscore = get_fscore(preds, tlabels)\n",
    "    fscore_max,freq = (fscore,25000000) if fscore > fscore_max else (fscore_max,freq)\n",
    "    \n",
    "    print(\"best freqency %d\"% freq)\n",
    "    dpred = frequency_threshold_feature(dwords, freq, counts)\n",
    "    tpred = frequency_threshold_feature(twords, freq,counts)\n",
    "    \n",
    "    dprecision,drecall,dfscore = get_precision(dpred,dlabels),get_recall(dpred,dlabels),get_fscore(dpred,dlabels)\n",
    "    tprecision,trecall,tfscore = get_precision(tpred,tlabels),get_recall(tpred,tlabels),get_fscore(tpred,tlabels)\n",
    "    \n",
    "    training_performance = [tprecision, trecall, tfscore]\n",
    "    development_performance = [dprecision, drecall, dfscore]\n",
    "    return training_performance, development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_frequency(counts, words):\n",
    "    max_freq = 0\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            freq = counts.get(word)\n",
    "            max_freq = freq if freq > max_freq else max_freq\n",
    "    return max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_min_frequency(counts, twords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_frequency(counts, words):\n",
    "    min_freq = 1000\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            freq = counts.get(word)\n",
    "            min_freq = freq if freq < min_freq else min_freq\n",
    "    return min_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_counts_file = \"ngram_counts.txt.gz\"\n",
    "counts = load_ngram_counts(ngram_counts_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best freqency 25000000\n",
      "([0.5450073782587309, 0.8562596599690881, 0.6660655244965434], [0.5457227138643068, 0.8851674641148325, 0.6751824817518248])\n"
     ]
    }
   ],
   "source": [
    "print(word_frequency_threshold(training_file,development_file,counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n"
     ]
    }
   ],
   "source": [
    "for key,val in counts.items():\n",
    "    if val == 1120679362:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(training_file, development_file, counts):\n",
    "    ## YOUR CODE HERE\n",
    "    twords, tlabels = load_file(training_file,True)\n",
    "    dwords, dlabels = load_file(development_file,True)\n",
    "    tlength = [ 0 if len(word) == None else len(word)for word in twords]\n",
    "    dlength = [ 0 if len(word) == None else len(word)for word in dwords]\n",
    "    tfreq = [ 0 if counts.get(w) == None else counts.get(w) for w in twords]\n",
    "    dfreq = [ 0 if counts.get(w) == None else counts.get(w) for w in dwords]\n",
    "    \n",
    "    tl = np.array(tlength)\n",
    "    meanl = np.mean(tl)\n",
    "    stdl  = np.std(tl)\n",
    "    tl_scale = [(l - meanl)/stdl for l in tl]\n",
    "    dl = np.array(dlength)\n",
    "    dl_scale = [(l - meanl)/stdl for l in dl]\n",
    "    tf = np.array(tfreq)\n",
    "    meanf = np.mean(tf)\n",
    "    stdf = np.std(tf)\n",
    "    tf_scale = [(f - meanf)/stdf for f in tf]\n",
    "    df = np.array(dfreq)\n",
    "    df_scale = [(f - meanf)/stdf for f in df]\n",
    "    X_train = np.matrix([tl_scale,tf_scale]).T\n",
    "    X_test = np.matrix([dl_scale,df_scale]).T\n",
    "    Y = tlabels\n",
    "    \n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, Y)\n",
    "    dpred = clf.predict(X_test)\n",
    "    tpred = clf.predict(X_train)\n",
    "    \n",
    "    tprecision,trecall,tfscore = get_precision(tpred,tlabels),get_recall(tpred,tlabels),get_fscore(tpred,tlabels)\n",
    "    dprecision,drecall,dfscore = get_precision(dpred,dlabels),get_recall(dpred,dlabels),get_fscore(dpred,dlabels)\n",
    "    training_performance = (tprecision, trecall, tfscore)\n",
    "    development_performance = (dprecision, drecall, dfscore)\n",
    "    return training_performance,development_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0.4918351477449456, 0.9775888717156105, 0.6544231764097258), (0.4700352526439483, 0.9569377990430622, 0.6304176516942475))\n"
     ]
    }
   ],
   "source": [
    "print(naive_bayes(training_file,development_file,counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(training_file, development_file, counts):\n",
    "    ## YOUR CODE HERE\n",
    "    twords, tlabels = load_file(training_file)\n",
    "    dwords, dlabels = load_file(development_file)\n",
    "    \n",
    "    tlength = [ 0 if len(word) == None else len(word)for word in twords]\n",
    "    dlength = [ 0 if len(word) == None else len(word)for word in dwords]\n",
    "    tfreq = [ 0 if counts.get(w) == None else counts.get(w) for w in twords]\n",
    "    dfreq = [ 0 if counts.get(w) == None else counts.get(w) for w in dwords]\n",
    "    \n",
    "    tl = np.array(tlength)\n",
    "    meanl = np.mean(tl)\n",
    "    stdl  = np.std(tl)\n",
    "    tl_scale = [(l - meanl)/stdl for l in tl]\n",
    "    dl = np.array(dlength)\n",
    "    dl_scale = [(l - meanl)/stdl for l in dl]\n",
    "    tf = np.array(tfreq)\n",
    "    meanf = np.mean(tf)\n",
    "    stdf = np.std(tf)\n",
    "    tf_scale = [(f - meanf)/stdf for f in tf]\n",
    "    df = np.array(dfreq)\n",
    "    df_scale = [(f - meanf)/stdf for f in df]\n",
    "    X_train = np.matrix([tl_scale,tf_scale]).T\n",
    "    X_test = np.matrix([dl_scale,df_scale]).T\n",
    "    Y = tlabels\n",
    "    \n",
    "    lr = LogisticRegression(C=1000.0, random_state=0, solver='liblinear')\n",
    "    lr.fit(X_train,Y)\n",
    "    tpred = lr.predict(X_train)\n",
    "    dpred = lr.predict(X_test)\n",
    "    tprecision,trecall,tfscore = get_precision(tpred,tlabels),get_recall(tpred,tlabels),get_fscore(tpred,tlabels)\n",
    "    dprecision,drecall,dfscore = get_precision(dpred,dlabels),get_recall(dpred,dlabels),get_fscore(dpred,dlabels)\n",
    "    \n",
    "    training_performance = (tprecision, trecall, tfscore)\n",
    "    development_performance = (dprecision, drecall, dfscore)\n",
    "    return development_performance, training_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0.7235142118863049, 0.6698564593301436, 0.6956521739130435), (0.7243478260869565, 0.6437403400309119, 0.6816693944353518))\n"
     ]
    }
   ],
   "source": [
    "print(logistic_regression(training_file, development_file, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syns_nums(words):\n",
    "    syns = []\n",
    "    for word in words:\n",
    "        counts = 0\n",
    "        for each in wordnet.synsets(word):\n",
    "            counts += len(each.lemma_names())\n",
    "        syns.append(counts)\n",
    "    return syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syllables(words):\n",
    "    return [syllables.count_syllables(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_senses(words):\n",
    "    return [len(wordnet.synsets(word)) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/weihaoran/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "twords, tlabels = load_file(training_file)\n",
    "dwords, dlabels = load_file(development_file)\n",
    "total_words = twords + dwords\n",
    "total_labels = tlabels + dlabels\n",
    "tsyns = regularization(get_syns_nums(twords))\n",
    "dsyns = regularization(get_syns_nums(dwords))\n",
    "tsyll = regularization(get_syllables(twords))\n",
    "dsyll = regularization(get_syllables(dwords))\n",
    "tsens = regularization(get_senses(twords))\n",
    "dsens = regularization(get_senses(dwords))\n",
    "syns = regularization(get_syns_nums(total_words))\n",
    "syll = regularization(get_syllables(total_words))\n",
    "sens = regularization(get_senses(total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization(paras):\n",
    "    data = np.array(paras)\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    return [(each - mean)/std for each in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlength = regularization([ 0 if len(word) == None else len(word)for word in twords])\n",
    "dlength = regularization([ 0 if len(word) == None else len(word)for word in dwords])\n",
    "tfreq = regularization([ 0 if counts.get(w) == None else counts.get(w) for w in twords])\n",
    "dfreq = regularization([ 0 if counts.get(w) == None else counts.get(w) for w in dwords])\n",
    "length = regularization([ 0 if len(word) == None else len(word)for word in total_words])\n",
    "freq = regularization([ 0 if counts.get(w) == None else counts.get(w) for w in total_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.matrix([tlength, tfreq, tsyns, tsyll, tsens]).T\n",
    "X_test = np.matrix([dlength, dfreq, dsyns, dsyll, dsens]).T\n",
    "X_total = np.matrix([length, freq, syns,syll,sens]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(gamma='scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_train, tlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpred = svc.predict(X_train)\n",
    "dpred = svc.predict(X_test)\n",
    "pred_svc = svc.predict(X_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "tprecision,trecall,tfscore = get_precision(tpred,tlabels),get_recall(tpred,tlabels),get_fscore(tpred,tlabels)\n",
    "dprecision,drecall,dfscore = get_precision(dpred,dlabels),get_recall(dpred,dlabels),get_fscore(dpred,dlabels)\n",
    "total_fscore_svc = get_fscore(pred_svc, total_labels)\n",
    "training_performance_svm = (tprecision, trecall, tfscore)\n",
    "development_performance_svm = (dprecision, drecall, dfscore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.6666666666666666, 0.8755980861244019, 0.7569803516028957),\n",
       " (0.7019011406844107, 0.7132921174652241, 0.7075507857416635),\n",
       " 0.7123446082682857)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "development_performance_svm,training_performance_svm, total_fscore_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(X_train,tlabels)\n",
    "tpred_knn = knn.predict(X_train)\n",
    "dpred_knn = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_dev = get_fscore(dpred_knn,dlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5895627644569816"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = tree.DecisionTreeClassifier()\n",
    "dtc.fit(X_train,tlabels)\n",
    "tpred_dtc = dtc.predict(X_train)\n",
    "dpred_dtc = dtc.predict(X_test)\n",
    "t_dtc = get_fscore(tpred_dtc, tlabels)\n",
    "d_dtc = get_fscore(dpred_dtc, dlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4384"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_dtc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=20, min_samples_split=100,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_estimators': range(300, 1100, 100)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test1 = {'n_estimators':range(300,1100,100)}\n",
    "gsearch1 = GridSearchCV(estimator = RandomForestClassifier(min_samples_split=100,\n",
    "                                  min_samples_leaf=20,max_depth=3,max_features='sqrt' ,random_state=0), \n",
    "                       param_grid = param_test1, scoring='f1',cv=5)\n",
    "gsearch1.fit(X_train,tlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=3, max_features='sqrt', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=20, min_samples_split=100,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=None,\n",
       "             oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       " 0.711022734623366,\n",
       " {'n_estimators': 400})"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.best_estimator_,gsearch1.best_score_,gsearch1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=20, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=None,\n",
       "            oob_score=True, random_state=0, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=False, n_jobs=None,\n",
       "       param_grid={'max_depth': range(1, 10)}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test2 = {'max_depth':range(1,10,1)}\n",
    "gsearch2 = GridSearchCV(estimator = RandomForestClassifier(n_estimators= 60, \n",
    "                                  min_samples_leaf=20,max_features='sqrt' ,oob_score=True, random_state=0),\n",
    "   param_grid = param_test2, scoring='f1',iid=False, cv=5)\n",
    "gsearch2.fit(X_train,tlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 7}, 0.7322393485136026)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=7, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=None,\n",
       "            oob_score=True, random_state=0, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=False, n_jobs=None,\n",
       "       param_grid={'min_samples_split': range(2, 10), 'min_samples_leaf': range(2, 60, 10)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='f1', verbose=0)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test3 = {'min_samples_split':range(2,10,1), 'min_samples_leaf':range(2,60,10)}\n",
    "gsearch3 = GridSearchCV(estimator = RandomForestClassifier(n_estimators= 400, max_depth=7,\n",
    "                                  max_features='sqrt' ,oob_score=True, random_state=0),\n",
    "   param_grid = param_test3, scoring='f1',iid=False, cv=5)\n",
    "gsearch3.fit(X_train,tlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=7, max_features='sqrt', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=2, min_samples_split=9,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=None,\n",
       "             oob_score=True, random_state=0, verbose=0, warm_start=False),\n",
       " 0.7329541119715254,\n",
       " {'min_samples_leaf': 2, 'min_samples_split': 9})"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch3.best_estimator_, gsearch3.best_score_,gsearch3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=7, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=2, min_samples_split=9,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=None,\n",
       "            oob_score=True, random_state=10, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=False, n_jobs=None,\n",
       "       param_grid={'max_features': range(1, 4)}, pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring='roc_auc', verbose=0)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " param_test4 = {'max_features':range(1,4,1)}\n",
    "gsearch4 = GridSearchCV(estimator = RandomForestClassifier(n_estimators= 400, max_depth=7, min_samples_split=9,\n",
    "                                  min_samples_leaf=2 ,oob_score=True, random_state=10),\n",
    "   param_grid = param_test4, scoring='roc_auc',iid=False, cv=5)\n",
    "gsearch4.fit(X_train,tlabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=7, max_features=2, max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=2, min_samples_split=9,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=None,\n",
       "             oob_score=True, random_state=10, verbose=0, warm_start=False),\n",
       " 0.8413455568882584,\n",
       " {'max_features': 2})"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch4.best_estimator_,gsearch4.best_score_,gsearch4.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=400, max_depth=3, random_state=0, max_features=3,oob_score=True)\n",
    "rfc.fit(X_train,tlabels)\n",
    "# tpred_rfc = rfc.predict(X_train)\n",
    "# dpred_rfc = rfc.predict(X_test)\n",
    "pred_rfc = rfc.predict(X_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_rfc = get_fscore(pred_rfc, total_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7117037037037036"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_rfc = get_fscore(tpred_rfc, tlabels)\n",
    "d_rfc = get_fscore(dpred_rfc, dlabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7377245508982035"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_rfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
